<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Santhisenan</title><link>https://santhisenan.github.io/tags/deep-learning/</link><description>Recent content in Deep Learning on Santhisenan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Â© Santhisenan A</copyright><lastBuildDate>Sat, 06 Apr 2024 11:44:59 +0800</lastBuildDate><atom:link href="https://santhisenan.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural Networks as Piecewise Linear Functions</title><link>https://santhisenan.github.io/posts/nn-as-piecewise-linear/</link><pubDate>Sat, 06 Apr 2024 11:44:59 +0800</pubDate><guid>https://santhisenan.github.io/posts/nn-as-piecewise-linear/</guid><description>Defining a simple neural network Today I learned that a simple shallow neural networks can be thought of as piecewise linear functions. Consider a simple neural network that maps a single scalar value, $x$ to a single scalar value $y$ , given by $$y = f[x, \theta]$$
Say this simple network only has 10 parameters, represented by $$\phi = {\phi_0, \phi_1, \phi_2, \phi_3, \theta_{10}, \theta_{11}, \theta_{20}, \theta_{21}, \theta_{30}, \theta_{31}}$$ and the equation</description></item></channel></rss>