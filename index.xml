<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Santhisenan</title><link>https://santhisenan.github.io/</link><description>Recent content on Santhisenan</description><generator>Hugo 0.125.1</generator><language>en-us</language><copyright>Â© Santhisenan A</copyright><lastBuildDate>Sat, 20 Apr 2024 11:31:28 +0800</lastBuildDate><atom:link href="https://santhisenan.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Approaches to generating local explanations in LLMs</title><link>https://santhisenan.github.io/posts/llm-explain-local/</link><pubDate>Sat, 20 Apr 2024 11:31:28 +0800</pubDate><guid>https://santhisenan.github.io/posts/llm-explain-local/</guid><description>Why care about explainability? Explaining why Large Language Models (LLMs) make a certain prediction is difficult. This is because LLMs are very complex &amp;ldquo;black box&amp;rdquo; models, i.e. their inner working mechanisms are opaque. However, there are mainly two reasons why we need to develop methods for explaining LLM predictions:
1. For end users of the models, explaining a model's predictions will help understand the reasoning behind a certain prediction, which can help build trust in the system they are using.</description></item><item><title>Intuition for deep neural networks</title><link>https://santhisenan.github.io/posts/intuition-dnns/</link><pubDate>Sat, 13 Apr 2024 10:03:13 +0800</pubDate><guid>https://santhisenan.github.io/posts/intuition-dnns/</guid><description>In this post, I will extend the idea of interpreting shallow neural networks as piecewise linear functions to deep neural networks. This post is based on chapter 4 of the Understanding Deep Learning textbook.
Composing two shallow neural networks Before looking into deep neural networks, let&amp;rsquo;s look at composing two shallow neural networks and see how the composition impacts the linear regions that are formed. Let&amp;rsquo;s define the first neural network that takes an input $x$ and returns an output $y$ by:</description></item><item><title>Neural Networks as Piecewise Linear Functions</title><link>https://santhisenan.github.io/posts/nn-as-piecewise-linear/</link><pubDate>Sat, 06 Apr 2024 11:44:59 +0800</pubDate><guid>https://santhisenan.github.io/posts/nn-as-piecewise-linear/</guid><description>Defining a simple neural network Today I learned that a simple shallow neural networks can be thought of as piecewise linear functions. Consider a simple neural network that maps a single scalar value, $x$ to a single scalar value $y$ , given by $$y = f[x, \theta]$$
Say this simple network only has 10 parameters, represented by $$\phi = {\phi_0, \phi_1, \phi_2, \phi_3, \theta_{10}, \theta_{11}, \theta_{20}, \theta_{21}, \theta_{30}, \theta_{31}}$$ and the equation</description></item><item><title>About</title><link>https://santhisenan.github.io/about/</link><pubDate>Sun, 13 Nov 2022 13:22:30 +0530</pubDate><guid>https://santhisenan.github.io/about/</guid><description>I am Master&amp;rsquo;s student at Nanyang Technological University. I work on segmentation of sub-cortical structures from brain MRI data.
I am supervised by Prof. Jagath Rajapakse.</description></item></channel></rss>