<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Global explanations in LLMs - Santhisenan</title><meta name=viewport content="width=device-width,initial-scale=1">
<meta itemprop=name content="Global explanations in LLMs"><meta itemprop=description content="A few techniques to generate global explanations for LLMs"><meta itemprop=datePublished content="2024-05-03T20:33:03+08:00"><meta itemprop=dateModified content="2024-05-03T20:33:03+08:00"><meta itemprop=wordCount content="654"><meta itemprop=keywords content="Deep Learning"><meta property="og:url" content="https://santhisenan.github.io/posts/global-explain-llm/"><meta property="og:site_name" content="Santhisenan"><meta property="og:title" content="Global explanations in LLMs"><meta property="og:description" content="A few techniques to generate global explanations for LLMs"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-03T20:33:03+08:00"><meta property="article:modified_time" content="2024-05-03T20:33:03+08:00"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Global explanations in LLMs"><meta name=twitter:description content="A few techniques to generate global explanations for LLMs"><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://santhisenan.github.io/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://santhisenan.github.io/js/main.js></script></head><body><div class="container wrapper"><div class=header><h1 class=site-title><a href=https://santhisenan.github.io/>Santhisenan</a></h1><div class=site-description><p>Working on Machine Learning</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/santhisenan title=Github><i data-feather=github></i></a></li><li><a href=https://www.linkedin.com/in/santhisenan/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/about>About</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>03</span>
<span class=rest>May 2024</span></div></div><div class=matter><h1 class=title>Global explanations in LLMs</h1></div></div><div class=markdown><p>Global explanations aim to offer insights into the inner workings of an LLM by understanding what individual components have encoded. Here, individual components could be neurons, hidden layers or even larger modules. In this post, we will look at four main methods &ndash; probing, neuron activation analysis, concept-based methods and mechanistic interpretation.</p><h2 id=probing-based-methods>Probing-based methods</h2><p>During self-supervised pre-training, LLMs acquire broad linguistic knowledge from training data. Using probing techniques, we can understand the knowledge that the LLMs have captured. There are two kinds of probing.</p><p>The first kind, called classifier-based probing, fits a shallow classifier on top of pre-trained or fine-tuned models.</p><ul><li>First, the parameters of the base model is frozen and the model generates representations for input words, phrases, or sentences.</li><li>These representations are fed into the probe classifier, which is used to identify certain linguistic properties of reasoning abilities acquired by the model.</li><li>After training, the probe is evaluated on a holdout dataset.</li></ul><p>The second type of probing, are data-centric methods called parameter-free probing does not require probing classifiers.</p><ul><li>First, a dataset are designed tailored to a specific property you want to test.</li><li>The base model&rsquo;s performance on this dataset give insights into it&rsquo;s capability in capturing this property.</li></ul><p>Data-driven prompt search is another technique where certain knowledge is examined via language model&rsquo;s text generation or completion capabilities.</p><h2 id=neuron-activation-explanation>Neuron Activation Explanation</h2><p>This technique tries to find methods to understand the activation patterns in a model. To identify the roles of specific neurons in a specific neurons in a model, particularly in relation to learning and processing linguistic properties, this simple method can be used.</p><ul><li>First, important neurons can be identified in an unsupervised manner. This can be done by identifying neurons that are consistently active across many inputs.</li><li>Relationship between these important neurons and particular linguistic properties can be identified using supervised tasks.</li><li>Most important neurons can be found by comparing the correlation metrics between their activations and the linguistic properties, or the weights learned during supervised training.</li></ul><h2 id=concept-based-explanation>Concept-Based Explanation</h2><p>Concept-based interpretability algorithms like Testing with Concept Activation Vectors (TCAVs) map the input data to a higher level, human-understandable concepts and explain the predictions of a model using the predefined concepts.</p><ul><li>First, we need to define a set of concepts that might be important for the model&rsquo;s decisions.</li><li>For each concept, we need to then find a group of examples that represent them. These examples are then used to train concept activation vectors (CAVs). CAVs are direction vectors in a model&rsquo;s embedding space that point in the direction of a concept.</li><li>TCAV then learns a linear classifier that acts as the CAV. This vector then serves as the boundary that differentiates between activations caused by the presence of a concept and those that are not related to it.</li><li>Once the CAV is established, TCAV uses directional derivatives to measure how changes in the direction of the input space affects the model&rsquo;s predictions. This is done by perturbing the input along the direction of CAV and seeing how the output changes.</li><li>The result of this is an importance score that defines quantifies the contribution of this concept to the model&rsquo;s predictions. A high TCAV score for a concept means that the model&rsquo;s predictions are sensitive to changes in the direction of this concept, indicating a high influence on predictions.</li></ul><p>The difficulty in defining useful concepts and the need to collect examples for each concept is a challenge in using concept-based explanations.</p><h2 id=mechanistic-interpretability>Mechanistic Interpretability</h2><p>Mechanistic interpretability focusses on dissecting and understanding the complex network of neurons and their interconnections. This helps to understand how specific components of the model contribute causally to it&rsquo;s behaviour and outputs.</p><p>In this post, we looked at a few techniques for generating global explanations for LLMs.
These are my notes from the following paper:</p><blockquote><p>H. Zhao <em>et al.</em>, “Explainability for Large Language Models: A Survey,” <em>ACM Trans. Intell. Syst. Technol.</em>, vol. 15, no. 2, p. 20:1-20:38, Feb. 2024, doi: <a href=https://doi.org/10.1145/3639372>10.1145/3639372</a>.</p></blockquote></div><div class=tags><ul class=flat><li><a href=/tags/deep-learning>deep-learning</a></li></ul></div></div></div><div class="footer wrapper"><nav class=nav><div>2024 © Santhisenan A | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script></body></html>