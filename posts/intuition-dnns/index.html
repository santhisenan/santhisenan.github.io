<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Intuition for deep neural networks - Santhisenan</title><meta name=viewport content="width=device-width,initial-scale=1">
<meta itemprop=name content="Intuition for deep neural networks"><meta itemprop=description content="Extending the piecewise linear functions intuition to a simple two-layer deep neural network."><meta itemprop=datePublished content="2024-04-13T10:03:13+08:00"><meta itemprop=dateModified content="2024-04-13T10:03:13+08:00"><meta itemprop=wordCount content="776"><meta itemprop=keywords content="Deep Learning"><meta property="og:url" content="https://santhisenan.github.io/posts/intuition-dnns/"><meta property="og:site_name" content="Santhisenan"><meta property="og:title" content="Intuition for deep neural networks"><meta property="og:description" content="Extending the piecewise linear functions intuition to a simple two-layer deep neural network."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-13T10:03:13+08:00"><meta property="article:modified_time" content="2024-04-13T10:03:13+08:00"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intuition for deep neural networks"><meta name=twitter:description content="Extending the piecewise linear functions intuition to a simple two-layer deep neural network."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://santhisenan.github.io/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://santhisenan.github.io/js/main.js></script></head><body><div class="container wrapper"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><div class=header><h1 class=site-title><a href=https://santhisenan.github.io/>Santhisenan</a></h1><div class=site-description><p>Working on Machine Learning</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/santhisenan title=Github><i data-feather=github></i></a></li><li><a href=https://www.linkedin.com/in/santhisenan/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/about>About</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>13</span>
<span class=rest>Apr 2024</span></div></div><div class=matter><h1 class=title>Intuition for deep neural networks</h1></div></div><div class=markdown><p>In this post, I will extend the idea of interpreting shallow neural networks as piecewise linear functions to deep neural networks. This post is based on chapter 4 of the <a href=https://udlbook.github.io/udlbook/>Understanding Deep Learning</a> textbook.</p><h2 id=composing-two-shallow-neural-networks>Composing two shallow neural networks</h2><p>Before looking into deep neural networks, let&rsquo;s look at composing two shallow neural networks and see how the composition impacts the linear regions that are formed. Let&rsquo;s define the first neural network that takes an input $x$ and returns an output $y$ by:</p><p>$$h_1 = a[\theta_{10} + \theta_{11}x]$$
$$h_2 = a[\theta_{20} + \theta_{21}x]$$
$$h_3 = a[\theta_{30} + \theta_{31}x]$$</p><p>and</p><p>$$y = \phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3$$</p><p>The second network takes $y$ , the output of previous network, as the input and produces $y&rsquo;$ and is defined by</p><p>$$h_1^\prime = a[\theta_{10}^\prime + \theta_{11}^\prime x]$$</p><p>$$h_2^\prime = a[\theta_{20}^\prime + \theta_{21}^\prime x]$$</p><p>$$h_3^\prime = a[\theta_{30}^\prime + \theta_{31}^\prime x]$$</p><p>and</p><p>$$y^\prime = \phi_0^\prime + \phi_1^\prime h_1^\prime + \phi_2^\prime h_2^\prime + \phi_3^\prime h_3^\prime$$</p><p>As described in the previous post, with ReLU activation functions, these networks can also be interpreted as piecewise linear functions as follows.
<img src=./images/image_1712969082329_0.png alt=image.png><br>The first network maps $x \in [-1, 1]$ to outputs $y \in [-1, 1]$ and since there are three hidden units, there are three linear regions. Also notice that multiple inputs $x$ can map to the same output $y$ for this function. This means that three different ranges of $x$ are mapped to the same output range $y$ . Similarly, the second network is also defined by three linear regions as shown. When we combine these two networks, the composed network will be defined by nine linear regions as shown below.
<img src=./images/image_1712969483016_0.png alt=image.png></p><h2 id=composition-as-a-special-case-of-deep-neural-networks>Composition as a special case of deep neural networks</h2><p>Composition of two neural networks is a special case of a deep neural network with two layers. We can expand the equations for the second neural network as follows:</p><p>$$h^\prime_1 = a[\theta^\prime_{10} + \theta^\prime_{11}x] = a[\theta^\prime_{10} + \theta^\prime_{11} \phi_0 + \theta^\prime_{11} \phi_1 h_1 + \theta^\prime_{11} \phi_2 h_2 + \theta^\prime_{11} \phi_3 h_3 ]$$</p><p>$$h^\prime_2 = a[\theta^\prime_{20} + \theta^\prime_{21}x] = a[\theta^\prime_{20} + \theta^\prime_{21} \phi_0 + \theta^\prime_{21} \phi_1 h_1 + \theta^\prime_{21} \phi_2 h_2 + \theta^\prime_{21} \phi_3 h_3 ]$$</p><p>$$h^\prime_3 = a[\theta^\prime_{30} + \theta^\prime_{31}x] = a[\theta^\prime_{30} + \theta^\prime_{31} \phi_0 + \theta^\prime_{31} \phi_1 h_1 + \theta^\prime_{31} \phi_2 h_2 + \theta^\prime_{31} \phi_3 h_3 ]$$</p><p>Let&rsquo;s rewrite this as</p><p>$$h^\prime_1 = a[\psi_{10} + \psi_{11} h_1 + \psi_{12} h_2 + \psi_{13} h_3]$$</p><p>$$h^\prime_2 = a[\psi_{20} + \psi_{21} h_1 + \psi_{22} h_2 + \psi_{23} h_3]$$</p><p>$$h^\prime_3 = a[\psi_{30} + \psi_{31} h_1 + \psi_{32} h_2 + \psi_{33} h_3 $$</p><p>The result is a network with two hidden layers. Therefore, it follows that a network with two layers can represent a family of functions created by passing the output of one single-layer network onto another.<br><img src=./images/image_1712973355203_0.png alt=image.png></p><p>However, a network with two layers represent a much broader family of functions. For the special case where a two layered network represents a composition of two neural networks, the parameters $\psi$ s take the value of $[\theta^\prime_{11}, \theta^\prime_{21}, \theta^\prime_{31}]^T[\phi_1, \phi_2, \phi_3]$ .</p><h2 id=deep-neural-networks>Deep neural networks</h2><p>Now consider the general case of a deep neural network with two hidden layers, each containing three hidden units each. The first layer is defined by:</p><p>$$h_1 = a[\theta_{10} + \theta_{11}x]$$</p><p>$$h_2 = a[\theta_{20} + \theta_{21}x]$$</p><p>$$h_3 = a[\theta_{30} + \theta_{31}x]$$</p><p>the second layer by:</p><p>$$h^\prime_1 = a[\psi_{10} + \psi_{11} h_1 + \psi_{12} h_2 + \psi_{13} h_3]$$</p><p>$$h^\prime_2 = a[\psi_{20} + \psi_{21} h_1 + \psi_{22} h_2 + \psi_{23} h_3]$$</p><p>$$h^\prime_3 = a[\psi_{30} + \psi_{31} h_1 + \psi_{32} h_2 + \psi_{33} h_3]$$</p><p>and the output by:</p><p>$$y^\prime = \phi^\prime_0 + \phi^\prime_1 h^\prime_1 + \phi^\prime_2 h^\prime_2 + \phi^\prime_3 h^\prime_3$$</p><p>Considering these equations leads to another way to think about how a neural network constructs an increasingly complicated function</p><ul><li>The three hidden units in the first layer, $h_1, h_2, h_3$ are computed as usual by linear functions of input and passing these through ReLU activation functions as shown below:
<img src=./images/image_1712971202704_0.png alt=image.png></li><li>The pre-activations at the second layer are computed by taking three new linear functions of these hidden units. At this point, we effectively have a shallow neural network with three outputs</li><li>At the second hidden layer, another ReLU function is applied to each function, which clips and adds new &ldquo;joints&rdquo; to each.
<img src=./images/image_1712971451302_0.png alt=image.png></li><li>The final output is a linear combination of these hidden units
<img src=./images/image_1712971487433_0.png alt=image.png></li></ul><p>We can think of each layer as creating new input functions, which are clipped (creating new regions) and then recombined.</p><p>We can extend the same idea of interpreting shallow neural networks as piecewise linear functions into the realm of deeper networks. I show an example using a simple two-layer network here. However, in practice, deep neural network has many more than two layers.</p></div><div class=tags><ul class=flat><li><a href=/tags/deep-learning>deep learning</a></li></ul></div></div></div><div class="footer wrapper"><nav class=nav><div>2024 Â© Santhisenan A | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script></body></html>