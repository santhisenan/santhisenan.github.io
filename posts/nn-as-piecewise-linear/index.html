<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Neural Networks as Piecewise Linear Functions - Santhisenan</title><meta name=viewport content="width=device-width,initial-scale=1">
<meta itemprop=name content="Neural Networks as Piecewise Linear Functions"><meta itemprop=description content="A simple neural network as a piecewise linear function."><meta itemprop=datePublished content="2024-04-06T11:44:59+08:00"><meta itemprop=dateModified content="2024-04-06T11:44:59+08:00"><meta itemprop=wordCount content="687"><meta itemprop=keywords content="Deep Learning"><meta property="og:url" content="https://santhisenan.github.io/posts/nn-as-piecewise-linear/"><meta property="og:site_name" content="Santhisenan"><meta property="og:title" content="Neural Networks as Piecewise Linear Functions"><meta property="og:description" content="A simple neural network as a piecewise linear function."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-06T11:44:59+08:00"><meta property="article:modified_time" content="2024-04-06T11:44:59+08:00"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Networks as Piecewise Linear Functions"><meta name=twitter:description content="A simple neural network as a piecewise linear function."><link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://santhisenan.github.io/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://santhisenan.github.io/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://santhisenan.github.io/js/main.js></script></head><body><div class="container wrapper"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><div class=header><h1 class=site-title><a href=https://santhisenan.github.io/>Santhisenan</a></h1><div class=site-description><p>Working on Machine Learning</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/santhisenan title=Github><i data-feather=github></i></a></li><li><a href=https://www.linkedin.com/in/santhisenan/ title=LinkedIn><i data-feather=linkedin></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/about>About</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>06</span>
<span class=rest>Apr 2024</span></div></div><div class=matter><h1 class=title>Neural Networks as Piecewise Linear Functions</h1></div></div><div class=markdown><h2 id=defining-a-simple-neural-network>Defining a simple neural network</h2><p>Today I learned that a simple shallow neural networks can be thought of as piecewise linear functions. Consider a simple neural network that maps a single scalar value, $x$ to a single scalar value $y$ , given by
$$y = f[x, \theta]$$</p><p>Say this simple network only has 10 parameters, represented by
$$\phi = {\phi_0, \phi_1, \phi_2, \phi_3, \theta_{10}, \theta_{11}, \theta_{20}, \theta_{21}, \theta_{30}, \theta_{31}}$$
and the equation<br>$$y = \phi_0 + \phi_1 a[\theta_{10} + \theta_{11}x] + \phi_2 a[\theta_{20} + \theta_{21}x] + \phi_3 a[\theta_{30} + \theta_{31}x]$$</p><p>This equation has three parts. First, we evaluate three linear functions of the input x, i.e. $\theta_{*0} + \theta_{*1}x$ . Then we pass the outputs of these linear functions through an activation function $a[\cdot]$ and finally, we take a weighted sum of the outputs of the activation function using $\phi_1, \phi_2, \phi_3$ as weights and add an offset $\phi_0$ .</p><p>A common choice for the activation function is the Rectified Linear Unit (ReLU) function. ReLU clips the negative values in it&rsquo;s input to zero.
<img src=./images/image_1.png alt=image.png></p><h2 id=thinking-about-the-parameters>Thinking about the parameters</h2><p>We can see that $f[x, \phi]$ represents a family of functions where each member in the family is defined by an element in the set of parameters $\phi$ . If we know these parameters, we can calculate the value of $y$ for any input $x$ by evaluating the function $f[x, \phi]$ . However, how can we find the parameters $\phi$ ?.</p><p>Given a training dataset, which contains the pairs of values for $x$ and $y$ , say ${x_i, y_i}_{i=1}^I$ , we can define a least squares loss function $L(\phi)$ and use this to measure how effectively $f$ estimates the training dataset. Once we have this function to measure this loss, we can find the optimal set of parameters $\hat{\phi}$ that minimises this loss.</p><h2 id=but-where-is-the-piecewise-linear-function>But, where is the piecewise linear function?</h2><p>The function $f[x, \phi]$ defines a piecewise linear function with upto 4 linear regions. To understand this, let us refactor the equation a bit. First, let&rsquo;s calculate these values $h_1, h_2, h_3$ , which we will call hidden units.
$$h_1 = a[\theta_{10} + \theta_{11} x]$$ $$h_2 = a[\theta_{20} + \theta_{21} x]$$ $$h_3 = a[\theta_{30} + \theta_{31} x]$$ Now, we can compute the output of $f[x, \theta]$ by computing a weighted sum of these hidden units<br>$$y = \phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3$$<br>Let&rsquo;s visualize this function to see how this is a piecewise linear function. To start with, the plots below represents the first step in this equation, where we calculate a linear function of the inputs, i.e. $\theta_{*0} + \theta_{*1}x$ .<br><img src=./images/image_2.png alt=image.png></p><p>The second step in calculating this equation is to apply the ReLU activation function to these outputs, which will clip the negative values to zero. These will be outputs of the hidden units:
<img src=./images/image_3.png alt=image.png><br>Now, let&rsquo;s perform the weighted sum using the parameters $\phi_*$
<img src=./images/image_4.png alt=image.png><br>Finally, we can add all these individual pieces together with an offset to produce the final function as shown below:
<img src=./images/image_5.png alt=image.png></p><p>We can see that the final function turns out to be a piecewise linear function with four segments. Each segment&rsquo;s slope is determined by the slope of the corresponding segment of the inputs that are active and the parameters $\phi_*$ these sections are scaled by. For example, in the shaded region, input 2 is not active, this it&rsquo;s slope is determined only by inputs 1 and 3.</p><p>Each hidden unit contributes one joint to the final function. So, with three hidden units there are four linear sections. However, only three of these sections are independent, since the fourth one is either zero or a function of the other three.</p><p>In today&rsquo;s post, we saw how a simple neural network with one scalar input and one scalar output can be thought of as a piecewise linear function. The number of linear segments is determined by the number of hidden layers and the parameters they are weighted by. This idea can be extended to deeper neural network, but that&rsquo;s for another day.</p><p>This blogpost is based on my notes from the Shallow neural networks chapter in the book <a href=https://udlbook.github.io/udlbook/>Understanding Deep Learning</a>.</p></div><div class=tags><ul class=flat><li><a href=/tags/deep-learning>deep learning</a></li></ul></div></div></div><div class="footer wrapper"><nav class=nav><div>2024 Â© Santhisenan A | <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script></body></html>